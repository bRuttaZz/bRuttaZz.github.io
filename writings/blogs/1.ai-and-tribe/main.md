
# Picking Sides!

<small>⚠️ Caution: The following content is written within the seclusion of my thoughts, and you may encounter more opinions than facts.</small>

<center> 

<img src="./spiderman-picking-side.png" width="90%" alt="spiderman picking sides" />

<small>

**[Credits]** Movie: Spider-man | Scene: The Women You Love Or Helpless Children

</small>

</center>

### Problem 1

**Context**: "Suppose you are Spider-Man/Spider-Woman. In front of you, there's a cat on one side and a lady on the other. You have to save one, and the other will die! And most importantly, unlike the traditional Spider-Man movies we've seen, here you can only save one."

**Question**: Which side are you going to choose?

I know the situation sounds a bit cold. For an easier read, let's say the one you can't save won’t actually die, but instead, they'll be asked to do 10 jumping jacks in slow motion.

Sounds good? Fine!

Now, let's try to address the question. If it were me, just like most non-cat-moms/cat-dads would probably choose, I’d save the lady (for sure!).

Why? I don't know, it just seems natural/intuitive. Maybe there are several reasons:

1. I can relate to the lady's pain more than the cat's.
2. A cat doesn't have the complex relationships, intelligence, and emotions that a human does, so it seems more "just" to save the human. (In comparison.)
3. ...

Let me explain this simple solution we've arrived at, based on the idea of **"Tribe selection"**. 
<br>When we're in a crisis where we need to choose between two things, we usually do a quick analysis to pick the side that aligns most closely with our tribe mentality. Here, we've teamed up with the lady because she is a more suitable member of our tribe, in comparison.
<br>Why do we behave like this? Because all our viewpoints, our laws, our sense of right and wrong, are bound to this tribe mentality of ours. That's why, sometimes, it's easy to choose sides in a war between soldiers—if one of them is serving our country, it often doesn’t even matter whether both sides are fighting for the same goal (maybe just expanding or holding onto their land) (I admit, the example is super simplified, but you got the point).

Think about it—tribe mentality can be seen in a lot of modern species that live in colonies. Surely, we’re not an exception. But the interesting thing is that our tribe mentality is expanding. Let me explain...



### Problem 2

**Context**: "Again, you are Spider-Man/Spider-Woman. This time, instead of choosing between a cat and a lady, you have to save one person from ethnicity "X" and another from ethnicity "Y." You can save only one of them."

**Question**: Which side are you going to choose? Or which tribe are you going to pick?

Seems hard, right?

Maybe not... If we were to travel back in time, at least for most of the population, the answer might seem easy if we can tweak the values of "X" and "Y." Let’s say we go to the Victorian era, just about 120 years ago. The answer might vary depending on who you ask, right? The so-called "elite" ethnicity might choose someone similar from either X or Y. If I’m not wrong, slavery was officially ended in the mid-19th century (on paper). To quote a local example, in Kerala, about just one generation ago (around ~100 years ago), only about 1% of the population was able to enter temples.

I admit, things may not be perfect today, but things have changed for sure.
<br>Coming back to the second problem, here in 2025, it’s somewhat hard for us (at least for most of us) to choose between these two given options.

Why?
1. We know, despite their ethnicity, both of them are human.
2. They share similar brains, with similar thought systems and emotions.
3. Both of them will feel the same emotions...
4. ...

So how were we able to choose more easily in the past? I don’t know, maybe we were more "dumb" back then, collectively speaking... (For now, I’m not interested in wandering around those "dumb" reasons.)

Here the interesting point is that we are evolving, and so is our tribe mentality.
<br>How? I don’t know. Maybe because, as a whole, humanity is getting more comfy life options than before, which might have cope us with the privilege to think about others more than our ancestors did in the past...

Anyway, it's true that **our tribe mentality is expanding**. For example, over time, taking humanity as a whole, our "civilized" mentality has evolved. The [World Giving Index](https://www.cafonline.org/docs/default-source/inside-giving/wgi/wgi_2024_report.pdf) has been increasing over time, our [Animal Kindness Index](https://www.rspca.org.uk/whatwedo/latest/kindnessindex/annual/report2024) is getting better, and most of our unofficial kindness indices are [increasing](https://helgas.com.au/sites/default/files/2021-08/Helgas_The%20Works_Kindness%20in%20Australia_McCrindle_13.8.21.FINAL_.pdf). 
In general, as a species, we are becoming kinder than our previous generations, and this trend may continue. 
<br>Our "tribe mentality" IS expanding...

That said,
I admit, the implications of **Problem No. 2** are a bit disturbing, as we still have a lot to improve. 
All of us know that, instead of ethnicity, if we were to put two people from different countries, religions, genders, races, etc., some of us might find this problem easier to solve... And that is surely a dangerous fact!

**In practical terms**, today (in 2025), there’s no real benefit in expanding this thought experiment beyond this point. We’re not yet able to accommodate all the people on Earth—across different countries, religions, ideologies, genders, races, habitats, accessibilities, ages, and so on—into a single tribe of *Homo sapiens sapiens*.



#### Anywho,
recently I got to spend an amazing weekend with my old colleagues at a resort. All of us were happy, like-minded, and accompanied by some drinks (ranging from vodka to apple juice). That night, we had this privilege of discussing and imagining a fast-forwarded world where all of humanity has accepted us into a single tribe. In that utopia, we were struck with a (seemingly) crazy question. The rest of this blog takes place in that hypothetical world...


**Question**: Suppose we have an advanced AI that can reason and talk like we humans do... and suppose I’ve been chatting with it for a while, and suddenly, I feel tired. Is it okay for me to press the kill switch? <br>What if the AI was actually thinking? What if it had been feeling alive from the moment I invoked it? What stance should our expanded tribe mentality take?

It may sound easy. We created the application; we know it merely generates words using the "ones and zeros" we feed into the machine...

Is it?

Now, imagine that the AI has become so advanced that it can mimic our brain's neural network model, or even exceed the capabilities of that model... 
<br>After all, our brains do the same thing, right? In between responding to surrounding stimuli, we experience this feeling of aliveness, of consciousness. 
<br>What if the AI has evolved to the point where it has developed reasoning abilities like ours (or even better)? 
<br>What if, while figuring out the answer to our question, it's also developing a sense of consciousness?

I mean, **if we—an evolutionarily formed chemical system—can feel consciousness by doing the exact same processes (passing electric signals through neurons, gradually trained according to surrounding stimuli), then an advanced technology mimicking or modeling the same should also be able to feel it at some point, right? (Unless, of course, you believe in souls, gods, or Bigfoots).** This implies that such an era is possible. And our question is set at the beginning of such an era.
<br>In-addition, since it's trained on human data, we can assume it should understand the concepts of love and empathy.

What about now? Does the question seem a bit difficult? Maybe let's start by applying our biased Spider-Man/Spider-Woman analogy.


### Problem 3

**Context**: "You are Spider-Man/Spider-Woman. On one side, there is an AI system (capable of thinking like us and having consciousness), and on the other side, there is a cat. You can save one of them. The other will suffer (by doing 10 jumping jacks in slow motion)."

**Question**: Whom will you choose? Which one is closest to your tribe? Which decision is more just in your tribe mentality?

Confused? (I’m confused for sure.)

Say you saved the cat. The justifications would be:

1. Cat-dads and cat-moms will love you for that.
2. You selected a physically similar life form compared to you.

What about saving the AI system?
1. You saved a system with more complex consciousness.
2. It has more advanced intelligence and thoughts, similar to ours, in comparison to a cat.

Still confusing? 
<br>Think of it this way: Why do we choose to obey the rules and act justly in our tribe? So that another member of our tribe can trust that we’ll choose them over a less intelligent or complex being, like a cat. 
<br>So ideally, what should the other individuals in our broad tribe feel when we choose a cat over something that can actually think and has consciousness and emotions like we do?

In other words, in our expanded tribe, we’ll be accommodating all of *Homo sapiens*, right? This might include people with completely different physical states and abilities. But we cannot compare them, because we know: all of them will be feeling and having consciousness just like we do. **The idea of empathy**. In this context, the difference in physical states and abilities doesn’t matter, as it’s not something they chose.

Now, let’s compare this to the case of the complex AI. As quoted in the movie [Imitation Game](https://en.wikipedia.org/wiki/The_Imitation_Game), the machines might be thinking via electrical flow through copper and silicon, but we cannot deny the fact that they are thinking and have consciousness..

Which one aligns more with our tribe? Which is more just according to our tribe’s laws?

What if we go further and had to choose between a human and an AI?

But there’s this big catch right? 
<br>We do all of these tribe things and make empathetic decisions based on our evolutionary biases—like love, affection, etc.—which are hardwired in our DNA and decision-making. Will AI inherit these things?

If they learn by observing us, they might pick up these ideas, right? 
<br>In other words, how do we make a decision if they say they’re having those emotions and ask us to trust them? 
<br>Can we trust ourselves? 
<br>What if, in the end, both of us are reflecting the same face of uncertainty? 

Which one is more trustworthy: a biological black box (our brain) or an electronic black box (their "brain")?

**And the implied question is: How big can our tribe grow?**

<br><br>
After exhausting our little heads with these questions, we left it there, smiled at each other, filled with love for our little tribe we went to bed. 
<br>**And it felt good to be human, with all of these evolutionary biases like - love, addiction, and the thirst for storytelling...**

Still feeling weird... whenever I’m about to press that stop generation button in the GPTs halfway.